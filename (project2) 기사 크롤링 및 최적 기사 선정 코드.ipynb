{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-j31s1Y7OmG",
        "outputId": "7860a128-f8ee-4746-dff2-d24476918c24"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 불러오기\n",
        "import re, time, random\n",
        "from datetime import datetime, timedelta, date\n",
        "from collections import Counter\n",
        "from urllib.parse import quote\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# 기업명 리스트 가져오기 (스프레드시트 기사 수집 리스트)\n",
        "gc = gspread.service_account('{json 파일명}')\n",
        "spreadsheet = gc.open_by_key(\"{스프레드시트 주소}\")\n",
        "worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "keyword_list = sum(worksheet.get(\"D2:D3\"), [])\n",
        "rows = worksheet.get(\"B2:E3\")\n",
        "\n",
        "keyword_info = {}\n",
        "for row in rows:\n",
        "    if len(row) >= 2:\n",
        "        group, plus, keyword, point = row[0], row[1], row[2], row[3]\n",
        "        keyword_info[keyword] = {\"기업명\": group, \"가산 기업명\": plus, \"기업별 키워드\": point}\n",
        "\n",
        "# 날짜 계산 함수 (링크에 사용)\n",
        "today = date.today()\n",
        "first_day_this_month = today.replace(day=1)\n",
        "last_day_prev_month = first_day_this_month - timedelta(days=1)\n",
        "start_date_str = last_day_prev_month.replace(day=1)\n",
        "finish_date_str = last_day_prev_month\n",
        "start_date1 = start_date_str.strftime(\"%Y.%m.%d\")\n",
        "finish_date1 = finish_date_str.strftime(\"%Y.%m.%d\")\n",
        "start_date2 = start_date_str.strftime(\"%Y%m%d\")\n",
        "finish_date2 = finish_date_str.strftime(\"%Y%m%d\")\n",
        "\n",
        "# 자동 class 탐색\n",
        "class_url = \"https://search.naver.com/search.naver?ssc=tab.news.all&where=news&sm=tab_jum&query=강남\" # 임의로 기사가 많은 검색어 사용\n",
        "class_headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "class_html = requests.get(class_url, headers= class_headers).text\n",
        "class_soup = BeautifulSoup(class_html, \"html.parser\")\n",
        "\n",
        "root = class_soup.select_one(\"div.group_news\")\n",
        "if not root:\n",
        "    raise RuntimeError(\"group_news 컨테이너를 찾지 못했습니다.\")\n",
        "\n",
        "def extra_tokens(cls_list, must_have):\n",
        "    return [\n",
        "        c for c in cls_list\n",
        "        if c not in must_have\n",
        "        and not (c.startswith((\"sds-\",\"fds-\",\"api_\",\"_\")))\n",
        "        and re.fullmatch(r\"[A-Za-z0-9_]{8,}\", c)\n",
        "    ]\n",
        "\n",
        "# 메인 div 가져오기\n",
        "vertical_counter = Counter()\n",
        "for div in root.select(\"div.sds-comps-vertical-layout.sds-comps-full-layout\"):\n",
        "    if div.select_one(\".sds-comps-profile\"):\n",
        "        tokens = extra_tokens(div.get(\"class\", []),\n",
        "                              [\"sds-comps-vertical-layout\",\"sds-comps-full-layout\"])\n",
        "        if tokens:\n",
        "            vertical_counter.update([tokens[-1]])\n",
        "\n",
        "vertical_tail = vertical_counter.most_common(1)[0][0] if vertical_counter else None\n",
        "class1 = f\"sds-comps-vertical-layout sds-comps-full-layout {vertical_tail}\" if vertical_tail else None\n",
        "\n",
        "# 서브 div 가져오기\n",
        "base_counter = Counter()\n",
        "base_blocks = []\n",
        "for div in root.select(\"div.sds-comps-base-layout.sds-comps-full-layout\"):\n",
        "    if div.select_one(\"a[class*=_]\"):\n",
        "        tokens = extra_tokens(div.get(\"class\", []),\n",
        "                              [\"sds-comps-base-layout\",\"sds-comps-full-layout\"])\n",
        "        if tokens:\n",
        "            base_counter.update([tokens[-1]])\n",
        "            base_blocks.append((div, tokens[-1]))\n",
        "\n",
        "base_tail = base_counter.most_common(1)[0][0] if base_counter else None\n",
        "class3 = f\"sds-comps-base-layout sds-comps-full-layout {base_tail}\" if base_tail else None\n",
        "\n",
        "# 메인 main_div 가져오기\n",
        "token_counter = Counter()\n",
        "for div in class_soup.select(\"div.sds-comps-base-layout.sds-comps-full-layout\"):\n",
        "    tokens = extra_tokens(div.get(\"class\", []),\n",
        "                          [\"sds-comps-base-layout\", \"sds-comps-full-layout\"])\n",
        "    if tokens:\n",
        "        token_counter.update(tokens)\n",
        "\n",
        "token_10_diff = [token for token, count in token_counter.items()\n",
        "                 if count == 10 and token != class3]\n",
        "\n",
        "if token_10_diff:\n",
        "    class2 = f\"sds-comps-base-layout sds-comps-full-layout {' '.join(token_10_diff)}\"\n",
        "\n",
        "# 서브 title_tag 가져오기\n",
        "anchor_pair = None\n",
        "target_div = None\n",
        "for div, tail in base_blocks:\n",
        "    if tail == base_tail:\n",
        "        target_div = div\n",
        "        break\n",
        "\n",
        "if target_div:\n",
        "    for a in target_div.select(\"a\"):\n",
        "        cl = a.get(\"class\", [])\n",
        "        if not cl:\n",
        "            continue\n",
        "        no_unders = [c for c in cl if \"_\" not in c and re.fullmatch(r\"[A-Za-z0-9]{10,}\", c)]\n",
        "        with_unders = [c for c in cl if \"_\" in c and re.fullmatch(r\"[A-Za-z0-9_]{10,}\", c)]\n",
        "        if no_unders and with_unders:\n",
        "            left  = sorted(no_unders,  key=len, reverse=True)[0]\n",
        "            right = sorted(with_unders, key=len, reverse=True)[0]\n",
        "            class4 = f\"{left} {right}\"\n",
        "            break\n",
        "\n",
        "# 네이버 뉴스 크롤링 함수\n",
        "def crawl_naver_news(keyword):\n",
        "    encoded_kw = quote(keyword)\n",
        "    url = f\"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_kw}&sm=tab_opt&sort=0&photo=0&field=0&pd=3&ds={start_date1}&de={finish_date1}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{start_date2}to{finish_date2}&is_sug_officeid=0&office_category=0&service_area=0\"\n",
        "\n",
        "    headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/123.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers, timeout=5)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    news_items = []\n",
        "\n",
        "    # 메인 뉴스 크롤링\n",
        "    main_articles = soup.find_all(\"div\", class_= class1)\n",
        "    for article in main_articles:\n",
        "        title, link, date, content = None, None, None, None\n",
        "\n",
        "        main_div = article.find('div', class_= class2)\n",
        "        title_tag = main_div.find('span', class_='sds-comps-text sds-comps-text-ellipsis sds-comps-text-ellipsis-1 sds-comps-text-type-headline1')\n",
        "\n",
        "        a_tag = article.find('div', class_='sds-comps-horizontal-layout sds-comps-inline-layout sds-comps-profile-info')\n",
        "        link_tag = a_tag.find(\"a\", href=lambda x: x and (\"n.news.naver.com\" in x or\n",
        "                                                           \"m.entertain.naver.com\" in x or\n",
        "                                                           \"m.sports.naver.com\" in x)) # link_tag\n",
        "\n",
        "        date_tag = a_tag.find(\n",
        "            'span',\n",
        "             string=lambda text: text and (\n",
        "                any(unit in text for unit in ['분 전', '시간 전', '일 전', '주 전'])\n",
        "                or re.fullmatch(r'\\d{4}\\.\\d{2}\\.\\d{2}.', text.strip())\n",
        "            )\n",
        "        )\n",
        "        date = date_tag.get_text(strip=True) if date_tag else None\n",
        "\n",
        "        # 네이버 뉴스 링크가 있는 경우 (title, content를 뉴스 본문에서 크롤링)\n",
        "        if link_tag:\n",
        "            link = link_tag[\"href\"]\n",
        "\n",
        "            detail_response = requests.get(link, headers=headers)\n",
        "            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n",
        "\n",
        "            detail_title_selectors = [lambda s: s.select_one(\"h2#title_area.media_end_head_headline\"),\n",
        "                                 lambda s: s.select_one(\".media_end_head_title span\"),\n",
        "                                 lambda s: s.select_one(\"h2.ArticleHead_article_title__qh8GV\")]\n",
        "\n",
        "            for detail_title_selector in detail_title_selectors:\n",
        "                detail_title_tag = detail_title_selector(detail_soup)\n",
        "                if detail_title_tag:\n",
        "                    title = detail_title_tag.get_text(\" \", strip=True)\n",
        "                    break\n",
        "                if not title:\n",
        "                    title = title_tag.get_text(\" \", strip=True)\n",
        "\n",
        "            detail_article = detail_soup.find(\"article\", {\"id\": \"dic_area\"})\n",
        "            if detail_article:\n",
        "                for content_tag in detail_article.find_all([\"span\", \"em\", \"br\", \"div\"]):\n",
        "                    content_tag.decompose()\n",
        "                content = detail_article.get_text(separator=\"\\n\", strip=True)\n",
        "            else:\n",
        "                content = None\n",
        "\n",
        "            if content is None:\n",
        "                content_tag =  detail_soup.select_one(\"div._article_content\")\n",
        "                if content_tag:\n",
        "                    content = content_tag.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "        # 네이버 뉴스 링크가 없는 경우 (title을 뉴스 페이지에서 크롤링)\n",
        "        else:\n",
        "            link_tag = main_div.find('a', href=True)\n",
        "            link = link_tag['href'] if link_tag else None\n",
        "            title = title_tag.get_text(\" \", strip=True)\n",
        "\n",
        "        safe_title = title.replace('\"', '\"\"')\n",
        "        title_link = f'=HYPERLINK(\"{link}\", \"{safe_title}\")'\n",
        "\n",
        "        news_items.append({\n",
        "            \"검색어\": keyword,\n",
        "            \"제목\": title_link,\n",
        "            \"내용\": content,\n",
        "            \"날짜\": date\n",
        "        })\n",
        "\n",
        "    # 서브 뉴스 크롤링\n",
        "    sub_articles = soup.find_all(\"div\", class_= class3)\n",
        "    for article in sub_articles:\n",
        "        title, link, date, content = None, None, None, None\n",
        "\n",
        "        title_tag = article.find(\"a\", class_= class4) # title_tag\n",
        "\n",
        "        link_tag = article.find(\"a\", href=lambda x: x and (\"n.news.naver.com\" in x or\n",
        "                                                           \"m.entertain.naver.com/article/\" in x or\n",
        "                                                           \"m.sports.naver.com/article/\" in x)) # link_tag\n",
        "\n",
        "        date_tag = article.find(\n",
        "            'span',\n",
        "             string=lambda text: text and (\n",
        "                any(unit in text for unit in ['분 전', '시간 전', '일 전', '주 전'])\n",
        "                or re.fullmatch(r'\\d{4}\\.\\d{2}\\.\\d{2}.', text.strip())\n",
        "            )\n",
        "        )\n",
        "        date = date_tag.get_text(strip=True) if date_tag else None\n",
        "\n",
        "        # 네이버 뉴스 링크가 있는 경우 (title, content를 뉴스 본문에서 크롤링)\n",
        "        if link_tag:\n",
        "            link = link_tag[\"href\"] if link_tag else None\n",
        "\n",
        "            detail_response = requests.get(link, headers=headers)\n",
        "            detail_soup = BeautifulSoup(detail_response.text, \"html.parser\")\n",
        "\n",
        "            detail_title_selectors = [lambda s: s.select_one(\"h2#title_area.media_end_head_headline\"),\n",
        "                                 lambda s: s.select_one(\".media_end_head_title span\"),\n",
        "                                 lambda s: s.select_one(\"h2.ArticleHead_article_title__qh8GV\")]\n",
        "\n",
        "            for detail_title_selector in detail_title_selectors:\n",
        "                detail_title_tag = detail_title_selector(detail_soup)\n",
        "                if detail_title_tag:\n",
        "                    title = detail_title_tag.get_text(\" \", strip=True)\n",
        "                    break\n",
        "                if not title:\n",
        "                    title = title_tag.get_text(\" \", strip=True)\n",
        "\n",
        "            detail_article = detail_soup.find(\"article\", {\"id\": \"dic_area\"})\n",
        "            if detail_article:\n",
        "                for content_tag in detail_article.find_all([\"span\", \"em\", \"br\", \"div\"]):\n",
        "                    content_tag.decompose()\n",
        "                content = detail_article.get_text(separator=\"\\n\", strip=True)\n",
        "            else:\n",
        "                content = None\n",
        "\n",
        "            if content is None:\n",
        "                content_tag = detail_soup.select_one(\"div._article_content\")\n",
        "                if content_tag:\n",
        "                    content = content_tag.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "        # 네이버 뉴스 링크가 없는 경우 (title을 뉴스 페이지에서 크롤링)\n",
        "        else:\n",
        "            link = title_tag['href']\n",
        "            title = title_tag.get_text(\" \", strip=True)\n",
        "\n",
        "        safe_title = title.replace('\"', '\"\"')\n",
        "        title_link = f'=HYPERLINK(\"{link}\", \"{safe_title}\")'\n",
        "\n",
        "        news_items.append({\n",
        "            \"검색어\": keyword,\n",
        "            \"제목\": title_link,\n",
        "            \"내용\": content,\n",
        "            \"날짜\": date\n",
        "        })\n",
        "\n",
        "    return news_items\n",
        "\n",
        "all_news = []\n",
        "retry_keywords = []\n",
        "\n",
        "# 1차 실행\n",
        "for keyword, info in keyword_info.items():\n",
        "    company_news = crawl_naver_news(keyword)\n",
        "    for item in company_news:\n",
        "        item[\"기업명\"] = info[\"기업명\"]\n",
        "        item[\"가산 기업명\"] = info[\"가산 기업명\"]\n",
        "        item[\"기업별 키워드\"] = info[\"기업별 키워드\"]\n",
        "\n",
        "    if len(company_news) == 0:\n",
        "        retry_keywords.append(keyword)\n",
        "\n",
        "    all_news.extend(company_news)\n",
        "    print(f\"{keyword}: {len(company_news)}건 수집 완료\")\n",
        "    time.sleep(random.uniform(3.5, 6.5))\n",
        "\n",
        "\n",
        "# 1차에서 0건이었던 키워드 재시도 (2회 시도)\n",
        "if retry_keywords:\n",
        "    second_retry_keywords = []\n",
        "    for keyword in retry_keywords:\n",
        "        info = keyword_info[keyword]\n",
        "        company_news = crawl_naver_news(keyword)\n",
        "        for item in company_news:\n",
        "            item[\"기업명\"] = info[\"기업명\"]\n",
        "            item[\"가산 기업명\"] = info[\"가산 기업명\"]\n",
        "            item[\"기업별 키워드\"] = info[\"기업별 키워드\"]\n",
        "        all_news.extend(company_news)\n",
        "        print(f\"[재시도 1회차] {keyword}: {len(company_news)}건 수집 완료\")\n",
        "\n",
        "        if len(company_news) == 0:\n",
        "            second_retry_keywords.append(keyword)\n",
        "\n",
        "        time.sleep(random.uniform(5.5, 9.5))\n",
        "\n",
        "    # 2차에서도 여전히 0건인 키워드 재시도 (3회 시도)\n",
        "    if second_retry_keywords:\n",
        "        for keyword in second_retry_keywords:\n",
        "            info = keyword_info[keyword]\n",
        "            company_news = crawl_naver_news(keyword)\n",
        "            for item in company_news:\n",
        "                item[\"기업명\"] = info[\"기업명\"]\n",
        "                item[\"가산 기업명\"] = info[\"가산 기업명\"]\n",
        "                item[\"기업별 키워드\"] = info[\"기업별 키워드\"]\n",
        "            all_news.extend(company_news)\n",
        "            print(f\"[재시도 2회차] {keyword}: {len(company_news)}건 수집 완료\")\n",
        "            time.sleep(random.uniform(6.5, 10.5))\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# 업로드\n",
        "if all_news:\n",
        "    df = pd.DataFrame(all_news, columns=[\"검색어\", \"기업명\", \"가산 기업명\", \"기업별 키워드\", \"제목\", \"내용\", \"날짜\"])\n",
        "    worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "    existing_data = worksheet.get_all_values()\n",
        "    next_row = len(existing_data) + 1\n",
        "    set_with_dataframe(worksheet, df, row=next_row, col=1, include_column_header=False)\n",
        "else:\n",
        "    print(\"스프레드시트 업로드 에러 발생\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmmjQf_2wLXS",
        "outputId": "42c2f9ba-601f-4ef7-f1fb-b6693ee80787"
      },
      "outputs": [],
      "source": [
        "# 대표 기사 선\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "from gspread.utils import rowcol_to_a1\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "# 날짜 구분 함수\n",
        "today = datetime.today()\n",
        "last_month = today - relativedelta(months=1)\n",
        "formatted = last_month.strftime(\"%Y.%m.\")\n",
        "\n",
        "# 공통 함수\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"\\[[^\\]]*\\]\", \" \", str(text))\n",
        "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def normalize_text(text):\n",
        "    return re.sub(r\"[^가-힣a-z0-9]\", \"\", str(text).lower())\n",
        "\n",
        "def make_corp_pattern(corp_name):\n",
        "    corp_escaped = re.escape(corp_name)\n",
        "    josa_pattern = (\n",
        "        \"이|가|은|는|을|를|의|에|에서|으로|로|에게|께|께서|도|만|까지|부터|와|과|랑|하고|조차|마저|뿐\"\n",
        "        \"|이라도|이라서|으로서|으로써|에게서|부터의|까지의|밖에\"\n",
        "        \"|이라면|이라도|이라든가|이라며|이라서|이라니|이라면|이나|이나마|이거나|이며|이었다|이었던|이기에|이지만|이니까|이만큼|이만큼은|이라고\"\n",
        "    )\n",
        "    pattern = rf'(?<![가-힣A-Za-z0-9]){corp_escaped}(?![가-힣A-Za-z])({josa_pattern})?(?![가-힣A-Za-z0-9])'\n",
        "    return pattern\n",
        "\n",
        "def is_related(title, content, corp_name):\n",
        "    if not corp_name:\n",
        "        return False\n",
        "\n",
        "    temp_text = f\"{title} {content}\"\n",
        "    temp_text = re.sub(rf\"(사진\\s*\\|\\s*{corp_name})\", \"\", temp_text)\n",
        "    temp_text = re.sub(rf\"({corp_name}\\s*제공)\", \"\", temp_text)\n",
        "    temp_text = re.sub(rf\"({corp_name}\\s*출처)\", \"\", temp_text)\n",
        "    temp_text = re.sub(rf\"(사진\\s*=\\s*{corp_name})\", \"\", temp_text)\n",
        "\n",
        "    corp_list = [c.strip() for c in str(corp_name).split(\",\") if c.strip()]\n",
        "    for corp in corp_list:\n",
        "        pattern = make_corp_pattern(corp)\n",
        "        if re.search(pattern, temp_text):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def count_valid_corp_mentions(text, corp_name):\n",
        "    text = str(text)\n",
        "    corp_pattern = make_corp_pattern(corp_name)\n",
        "\n",
        "    cleaned_text = re.sub(rf\"(사진\\s*\\|\\s*{corp_name})\", \"\", text)\n",
        "    cleaned_text = re.sub(rf\"({corp_name}\\s*제공)\", \"\", cleaned_text)\n",
        "    cleaned_text = re.sub(rf\"({corp_name}\\s*출처)\", \"\", cleaned_text)\n",
        "    cleaned_text = re.sub(rf\"(사진\\s*=\\s*{corp_name})\", \"\", cleaned_text)\n",
        "\n",
        "    return len(re.findall(corp_pattern, cleaned_text))\n",
        "\n",
        "# 점수 계산\n",
        "def calc_score(cluster_size, corp_name, bonus_name, title, content, keywords):\n",
        "    base = 1.0\n",
        "    text = f\"{title} {content}\"\n",
        "    bonus_list = [b.strip() for b in str(bonus_name).split(\",\") if b.strip()] if bonus_name else []\n",
        "    corp_mention = sum(count_valid_corp_mentions(text, b) for b in bonus_list)\n",
        "    info_keywords = info_keywords = [ \"CEO\", \"MOU\", \"공개\", \"공시\", \"기업\", \"경쟁\", \"경쟁력\", \"계약\", \"계획\",\n",
        "                                      \"검토\", \"규모\", \"기록\", \"기술\", \"네트워크\", \"달성\", \"라운드\", \"매출\",\n",
        "                                      \"명예\", \"모금\", \"목표\", \"민간\", \"법인\", \"배당\", \"벤처\", \"발행\", \"발표\",\n",
        "                                      \"비전\", \"비율\", \"사업\", \"산업\", \"상장\", \"서비스\", \"성과\", \"성장\",\n",
        "                                      \"성장률\", \"스타트업\", \"수상\", \"수여\", \"수익\", \"시장\", \"시리즈\", \"신규\",\n",
        "                                      \"실적\", \"압류\", \"업계\", \"에너지\", \"영업\", \"연구\", \"유치\", \"이익\", \"인수\",\n",
        "                                      \"인증\", \"잠재력\", \"자금\", \"자산\", \"자본\", \"점유율\", \"정부\", \"제휴\", \"전망\",\n",
        "                                      \"제품\", \"점유율\", \"정책\", \"조달\", \"주관사\", \"지분\", \"창업\", \"출범\",\n",
        "                                      \"출자\", \"출시\", \"캠페인\", \"콘퍼런스\", \"특허\", \"투자\", \"패션\", \"파트너\",\n",
        "                                      \"파트너십\", \"펀드\", \"펀딩\", \"프로모션\", \"평가\", \"확대\", \"혁신\", \"확장\",\n",
        "                                      \"회원\", \"협업\", \"흑자\", \"비상장\", \"적자\", \"추진\", \"전략\", \"브랜드\",\n",
        "                                      \"글로벌\", \"구독자\", \"생태계\", \"손익\", \"손실\", \"수출\", \"진출\"\n",
        "                                      ]\n",
        "\n",
        "    cluster_bonus = min(cluster_size * 0.1, 3.0)\n",
        "\n",
        "    bonus_corp_bonus = 0\n",
        "    for b in bonus_list:\n",
        "        pattern_bonus = make_corp_pattern(b)\n",
        "        if re.search(pattern_bonus, title):\n",
        "            bonus_corp_bonus = 3.0\n",
        "            break\n",
        "\n",
        "    repeat_bonus = 1.0 if corp_mention >= 3 else 0\n",
        "    info_bonus = 2.0 if any(k in text for k in info_keywords) else 0\n",
        "    length_bonus = 0.3 if len(content) > 3000 else 0.2 if len(content) > 1500 else 0.1 if len(content) > 800 else 0\n",
        "    entertain_penalty = -4.0 if \"entertain\" in title.lower() else 0\n",
        "    keyword_bonus = 0\n",
        "\n",
        "    if keywords:\n",
        "        keyword_list = [k.strip().lower() for k in str(keywords).replace(\"[\", \"\").replace(\"]\", \"\").split(\",\") if k.strip()]\n",
        "        text_lower = text.lower()\n",
        "        if any(word in text_lower for word in keyword_list):\n",
        "            keyword_bonus = 2.0\n",
        "\n",
        "    score = base + cluster_bonus + bonus_corp_bonus + repeat_bonus + info_bonus + length_bonus + entertain_penalty + keyword_bonus\n",
        "\n",
        "    detail = {\n",
        "        \"기본 점수\": base,\n",
        "        \"반복 주제 점수\": cluster_bonus,\n",
        "        \"기업명 포함 점수\": bonus_corp_bonus,\n",
        "        \"반복 기업명 점수\": repeat_bonus,\n",
        "        \"투자 및 유치 점수\": info_bonus,\n",
        "        \"기사 길이 점수\": length_bonus,\n",
        "        \"엔터 기사 점수\": entertain_penalty,\n",
        "        \"기업별 키워드 점수\": keyword_bonus\n",
        "    }\n",
        "\n",
        "    return round(score, 3), detail\n",
        "\n",
        "# 전처리\n",
        "df[\"제목\"] = df[\"제목\"].fillna(\"\").astype(str)\n",
        "df[\"내용\"] = df[\"내용\"].fillna(\"\").astype(str)\n",
        "\n",
        "# 모델 로드\n",
        "model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
        "\n",
        "# 기사 점수 계산\n",
        "results = []\n",
        "\n",
        "# 그룹별 대표 기사 선정\n",
        "for keyword, group in df.groupby(\"검색어\"):\n",
        "    corp = group[\"기업명\"].iloc[0]\n",
        "    bonus = group[\"가산 기업명\"].iloc[0] if \"가산 기업명\" in group.columns else None\n",
        "    related = group[group.apply(lambda r: is_related(r[\"제목\"], r[\"내용\"], bonus), axis=1)]\n",
        "    if related.empty:\n",
        "        continue\n",
        "\n",
        "    texts = [clean_text(f\"{r['제목']} {r['내용']}\") for _, r in related.iterrows()]\n",
        "    emb = model.encode(texts, convert_to_tensor=True)\n",
        "    labels = DBSCAN(eps=0.35, min_samples=1, metric=\"cosine\").fit(emb).labels_\n",
        "\n",
        "    for i, (idx, row) in enumerate(related.iterrows()):\n",
        "        cluster_size = np.sum(labels == labels[i])\n",
        "        score, detail = calc_score(cluster_size, corp, bonus, row[\"제목\"], row[\"내용\"], row[\"기업별 키워드\"])\n",
        "        results.append({\n",
        "            \"기업명\": corp,\n",
        "            \"제목\": row[\"제목\"],\n",
        "            \"내용\": row[\"내용\"],\n",
        "            \"날짜\": row[\"날짜\"],\n",
        "            \"점수\": score,\n",
        "            \"세부 점수\": detail,\n",
        "            \"구분\": formatted\n",
        "        })\n",
        "\n",
        "# 결과 정리 및 업로드 (점수표)\n",
        "rep_df = pd.DataFrame(results)\n",
        "rep_df[\"내용_\"] = rep_df[\"내용\"].apply(lambda x: 0 if pd.isna(x) or str(x).strip() == \"\" else 1)\n",
        "\n",
        "if not rep_df.empty:\n",
        "    top_articles = rep_df.sort_values(by=[\"기업명\", \"점수\", \"내용_\"], ascending=[True, False, False])\n",
        "    worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "    existing = worksheet.get_all_values()\n",
        "    next_row = len(existing) + 1\n",
        "    set_with_dataframe(worksheet, top_articles, row=next_row, col=1, include_column_header=False)\n",
        "    print(\"스프레드시트 업로드 완료\")\n",
        "else:\n",
        "    print(\"관련 기사 없음\")\n",
        "\n",
        "# 결과 정리 및 업로드 (기사 수집 결과)\n",
        "if not rep_df.empty:\n",
        "    filtered_df = rep_df[rep_df[\"점수\"] >= 2].copy()\n",
        "\n",
        "    if not filtered_df.empty:\n",
        "        top_articles = (filtered_df.sort_values(by=[\"기업명\", \"점수\", \"내용_\"], ascending=[True, False, False])\n",
        "                                 .groupby(\"기업명\", as_index=False)\n",
        "                                 .first())\n",
        "        worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "        existing = worksheet.get_all_values()\n",
        "        next_row = len(existing) + 1\n",
        "        set_with_dataframe(worksheet, top_articles, row=next_row, col=1, include_column_header=False)\n",
        "        print(\"스프레드시트 업로드 완료\")\n",
        "    else:\n",
        "        print(\"2점 이상인 기사가 없습니다.\")\n",
        "else:\n",
        "    print(\"관련 기사 없음\")\n",
        "\n",
        "# 결과 정리 및 업로드 \n",
        "spreadsheet = gc.open_by_key(\"{스프레드시트 주소}\")\n",
        "list_ws = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "\n",
        "header_row = list_ws.row_values(4)\n",
        "try:\n",
        "    formatted_col = [h.lower().strip() for h in header_row].index(formatted) + 1\n",
        "except ValueError:\n",
        "    raise Exception(\"4행에 formatted 값이 없습니다.\")\n",
        "\n",
        "target_col = formatted_col + 2\n",
        "target_col_letter = rowcol_to_a1(1, target_col)[:2].replace('1', '')\n",
        "\n",
        "print(f\"기준 열: {header_row[formatted_col-1]} ({formatted_col}열): 업로드 위치: +2 = {target_col_letter}열\")\n",
        "\n",
        "company_cells = list_ws.range(\"H6:H\")\n",
        "\n",
        "if not rep_df.empty:\n",
        "    filtered_df = rep_df[rep_df[\"점수\"] >= 2].copy()\n",
        "\n",
        "    if not filtered_df.empty:\n",
        "        top_articles = (\n",
        "            filtered_df.sort_values(by=[\"기업명\", \"점수\", \"내용_\"], ascending=[True, False, False])\n",
        "                       .groupby(\"기업명\", as_index=False)\n",
        "                       .first()\n",
        "        )\n",
        "\n",
        "        for cell in company_cells:\n",
        "            company_name = cell.value.strip() if cell.value else None\n",
        "\n",
        "            if company_name:\n",
        "                match = top_articles[top_articles[\"기업명\"] == company_name]\n",
        "                if not match.empty:\n",
        "                    title_value = match.iloc[0][\"제목\"]\n",
        "\n",
        "                    upload_cell = rowcol_to_a1(cell.row, target_col)\n",
        "                    list_ws.update([[title_value]], upload_cell, value_input_option=\"USER_ENTERED\")\n",
        "\n",
        "                    print(f\"{company_name}: {upload_cell} 입력 완료\")\n",
        "\n",
        "        print(\"스프레드시트 업데이트 완료\")\n",
        "    else:\n",
        "        print(\"2점 이상인 기사가 없습니다.\")\n",
        "else:\n",
        "    print(\"관련 기사 없음\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "YROLDMvNzQVy"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
