{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27ddc51f",
      "metadata": {
        "id": "27ddc51f",
        "outputId": "c7e5896e-3915-4c8a-f3e5-a06ad2f7d051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nest-asyncio in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.0)\n",
            "Requirement already satisfied: cloudscraper in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.71)\n",
            "Requirement already satisfied: requests in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.14.2)\n",
            "Requirement already satisfied: lxml in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (6.0.2)\n",
            "Requirement already satisfied: chardet in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (5.2.0)\n",
            "Requirement already satisfied: pandas in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.3.4)\n",
            "Requirement already satisfied: openpyxl in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
            "Requirement already satisfied: xlrd in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.2)\n",
            "Requirement already satisfied: gspread in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (6.2.1)\n",
            "Requirement already satisfied: gspread-dataframe in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.0.0)\n",
            "Requirement already satisfied: oauth2client in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.1.3)\n",
            "Requirement already satisfied: openai in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.6.1)\n",
            "Requirement already satisfied: selenium in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.38.0)\n",
            "Requirement already satisfied: webdriver-manager in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.0.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (7.1.2)\n",
            "Requirement already satisfied: tqdm in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.67.1)\n",
            "Requirement already satisfied: python-dotenv in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cloudscraper) (3.2.5)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cloudscraper) (1.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gspread) (2.41.1)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gspread) (1.2.3)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gspread-dataframe) (1.17.0)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.31.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from oauth2client) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: trio<1.0,>=0.31.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.32.0)\n",
            "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (0.12.2)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.12.0->gspread) (6.2.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth-oauthlib>=0.4.1->gspread) (2.0.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: outcome in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio<1.0,>=0.31.0->selenium) (2.0.0)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from cffi>=1.14->trio<1.0,>=0.31.0->selenium) (2.23)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mobidays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.3.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 25.3\n",
            "[notice] To update, run: C:\\Users\\mobidays\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install nest-asyncio cloudscraper requests beautifulsoup4 lxml chardet \\\n",
        "pandas numpy openpyxl xlrd \\\n",
        "gspread gspread-dataframe oauth2client \\\n",
        "openai \\\n",
        "selenium webdriver-manager \\\n",
        "psutil \\\n",
        "tqdm \\\n",
        "python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0800a10",
      "metadata": {
        "id": "a0800a10",
        "outputId": "d1e85eff-aeb1-421d-9ee4-7a3bc862af74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== METAS ===\n",
            "{'site': 'Marketing Mag', 'url': 'https://www.marketingmag.com.au/', 'count_title': 14, 'count_link': 14, 'count_date': 14, 'count_content': 1, 'matched': 1, 'errors': [\"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\"]}\n",
            "{'site': 'Marketing Beat', 'url': 'https://www.marketing-beat.co.uk/', 'count_title': 18, 'count_link': 18, 'count_date': 18, 'count_content': 2, 'matched': 2, 'errors': []}\n",
            "{'site': 'Search Engine Land', 'url': 'https://searchengineland.com/', 'count_title': 40, 'count_link': 40, 'count_date': 40, 'count_content': 12, 'matched': 12, 'errors': []}\n",
            "{'site': 'ADWEEK', 'url': 'https://www.adweek.com/brand-marketing/', 'count_title': 10, 'count_link': 10, 'count_date': 10, 'count_content': 5, 'matched': 5, 'errors': []}\n",
            "{'site': 'Marketingnews', 'url': 'https://www.marketingnews.es/', 'count_title': 24, 'count_link': 24, 'count_date': 21, 'count_content': 7, 'matched': 7, 'errors': []}\n",
            "{'site': 'Euronews', 'url': 'https://www.euronews.com/business', 'count_title': 4, 'count_link': 4, 'count_date': 4, 'count_content': 4, 'matched': 4, 'errors': [\"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\", \"개별 기사 오류: 'NoneType' object has no attribute 'get'\"]}\n",
            "{'site': 'The Next Web', 'url': 'https://thenextweb.com/latest', 'count_title': 10, 'count_link': 10, 'count_date': 10, 'count_content': 0, 'matched': 0, 'errors': []}\n",
            "{'site': 'Social Media Today', 'url': 'https://www.socialmediatoday.com/', 'count_title': 25, 'count_link': 25, 'count_date': 25, 'count_content': 5, 'matched': 5, 'errors': []}\n",
            "{'site': 'EIN Presswire', 'url': 'https://www.einpresswire.com/channel/media-advertising-pr#', 'count_title': 20, 'count_link': 20, 'count_date': 20, 'count_content': 19, 'matched': 19, 'errors': []}\n",
            "{'site': 'AI NEWS', 'url': 'https://www.artificialintelligence-news.com/all-categories/', 'count_title': 18, 'count_link': 18, 'count_date': 18, 'count_content': 4, 'matched': 4, 'errors': []}\n",
            "{'site': 'MarketingTechNews', 'url': 'https://www.marketingtechnews.net/', 'count_title': 30, 'count_link': 30, 'count_date': 30, 'count_content': 0, 'matched': 0, 'errors': []}\n"
          ]
        }
      ],
      "source": [
        "# \"{}\" 안의 값은 변경하여 사용\n",
        "# 크롤링 프로그램\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import asyncio\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import cloudscraper\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import gspread\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "import re\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3 import PoolManager\n",
        "import ssl\n",
        "import chardet\n",
        "import random\n",
        "\n",
        "import warnings\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "# =========================\n",
        "# 날짜\n",
        "# =========================\n",
        "yesterday = datetime.today() - timedelta(days=1)\n",
        "yesterday_str = yesterday.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# =========================\n",
        "# 공통\n",
        "# =========================\n",
        "user_agents = [\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64)\"\n",
        "]\n",
        "\n",
        "headers = {\"User-Agent\": random.choice(user_agents)}\n",
        "\n",
        "def create_scraper():\n",
        "    return cloudscraper.create_scraper(\n",
        "        browser={\"browser\": \"firefox\", \"platform\": \"windows\", \"mobile\": False}\n",
        "    )\n",
        "\n",
        "_domain_locks = {}\n",
        "_domain_locks_lock = threading.Lock()\n",
        "\n",
        "def _get_domain_lock(domain: str):\n",
        "    with _domain_locks_lock:\n",
        "        if domain not in _domain_locks:\n",
        "            _domain_locks[domain] = threading.Semaphore(1)\n",
        "        return _domain_locks[domain]\n",
        "\n",
        "def _jitter_sleep(a=0.05, b=0.2):\n",
        "    time.sleep(random.uniform(a, b))\n",
        "\n",
        "def robust_get(scraper, url, headers, retries=2, backoff=0.6, min_len=300, timeout=10):\n",
        "    last_exc = None\n",
        "    dom = urlparse(url).netloc\n",
        "    lock = _get_domain_lock(dom)\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            with lock:\n",
        "                _jitter_sleep()\n",
        "                r = scraper.get(url, headers=headers, timeout=timeout)\n",
        "            if r is not None and r.status_code in (200, 201) and len(r.text) >= min_len:\n",
        "                return r\n",
        "        except Exception as e:\n",
        "            last_exc = e\n",
        "        time.sleep((backoff ** i) + random.uniform(0, 0.2))\n",
        "    if last_exc:\n",
        "        raise last_exc\n",
        "    return None\n",
        "\n",
        "def should_warn(meta: dict) -> bool:\n",
        "    must_fields = [\"count_title\", \"count_link\", \"count_date\"]\n",
        "\n",
        "    if any(meta.get(f, 0) == 0 for f in must_fields):\n",
        "        return True\n",
        "\n",
        "    matched = meta.get(\"matched\", 0)\n",
        "    content = meta.get(\"count_content\", 0)\n",
        "    if matched > 0 and content == 0:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def make_warning_row(meta: dict, date_str: str) -> pd.DataFrame:\n",
        "    return pd.DataFrame([{\n",
        "        \"매체명\": meta.get(\"site\", \"UNKNOWN\"),\n",
        "        \"제목\": \"⚠ 클래스 변경 가능성\",\n",
        "        \"날짜\": date_str,\n",
        "        \"내용\": (\n",
        "            f\"제목:{meta.get('count_title', 0)} \"\n",
        "            f\"링크:{meta.get('count_link', 0)} \"\n",
        "            f\"날짜:{meta.get('count_date', 0)} \"\n",
        "            f\"본문:{meta.get('count_content', 0)} \"\n",
        "            f\"매칭:{meta.get('matched', 0)} → 구조 점검 필요\"\n",
        "        ),\n",
        "        \"링크\": meta.get(\"url\", \"\")\n",
        "    }])\n",
        "\n",
        "\n",
        "class TLSAdapter(HTTPAdapter):\n",
        "    def init_poolmanager(self, *args, **kwargs):\n",
        "        context = ssl.create_default_context()\n",
        "        context.set_ciphers('HIGH:!DH:!aNULL')\n",
        "        kwargs['ssl_context'] = context\n",
        "        return super(TLSAdapter, self).init_poolmanager(*args, **kwargs)\n",
        "\n",
        "# =========================\n",
        "# 크롤러\n",
        "# =========================\n",
        "\n",
        "def crawl_marketingmag_sync():\n",
        "    site, url = \"Marketing Mag\", \"https://www.marketingmag.com.au/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(robust_get(scraper, url, headers).text, \"html.parser\")\n",
        "        for art in soup.find_all(\"div\", class_=\"tt-post-info\"):\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                title_tag = art.find(\"a\", class_=\"tt-post-title c-h4\");   title = title_tag.get_text(strip=True) if title_tag else None\n",
        "                link = title_tag.get(\"href\")\n",
        "                if not link: continue\n",
        "                date_tag = art.find(\"span\", class_=\"tt-post-date\");       date_raw = date_tag.get_text(strip=True) if date_tag else None\n",
        "                date = datetime.strptime(date_raw, \"%B %d, %Y\").strftime(\"%Y-%m-%d\") if date_raw else None\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner = BeautifulSoup(robust_get(scraper, link, headers).text, \"html.parser\")\n",
        "                    content_div = inner.select_one(\"div.simple-text.size-4.tt-content.title-droid.margin-big\")\n",
        "                    texts = [p.get_text(\" \", strip=True) for p in content_div.find_all([\"p\",\"h3\",\"blockquote\"])] if content_div else []\n",
        "                    if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\":0, \"count_link\":0,\n",
        "                                \"count_date\":0, \"count_content\":0, \"matched\":0, \"errors\":[f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_marketingbeat_sync():\n",
        "    site, url = \"Marketing Beat\", \"https://www.marketing-beat.co.uk/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(robust_get(scraper, url, headers).text, \"html.parser\")\n",
        "        for art in soup.find_all(\"li\", class_=\"mb-latest-articles-box\"):\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                link_tag = art.find(\"a\", class_=\"mb-articles-content-link\")\n",
        "                if not link_tag: continue\n",
        "                title_tag = link_tag.find(\"h3\")\n",
        "                title = title_tag.get_text(strip=True) if title_tag else None\n",
        "                link = link_tag.get(\"href\")\n",
        "\n",
        "                date_tag = link_tag.find(\"div\", class_=\"mb-articles-author-date\")\n",
        "                date_raw = date_tag.find(\"span\").get_text(strip=True) if date_tag and date_tag.find(\"span\") else None\n",
        "                date = None\n",
        "                if date_raw:\n",
        "                    try:\n",
        "                        date = datetime.strptime(date_raw.split(\"x\")[0].strip(), \"%d/%m/%Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception:\n",
        "                        date = date_raw\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner = BeautifulSoup(robust_get(scraper, link, headers).text, \"html.parser\")\n",
        "                    content_div = inner.select_one(\"div.mb-post-content\")\n",
        "                    texts = [p.get_text(\" \", strip=True) for p in content_div.find_all([\"p\",\"h2\",\"h3\",\"li\"])] if content_div else []\n",
        "                    if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_searchengine_sync():\n",
        "    site, url = \"Search Engine Land\", \"https://searchengineland.com/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(robust_get(scraper, url, headers).text, \"html.parser\")\n",
        "        for art in soup.select(\"article.stream-article\"):\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                a_tag = art.select_one(\"h2.headline a\")\n",
        "                if not a_tag: continue\n",
        "                title = a_tag.get_text(strip=True)\n",
        "                link = a_tag.get(\"href\")\n",
        "\n",
        "                date_tag = art.select_one(\".author-time .byline\")\n",
        "                date_raw = date_tag.get_text(strip=True).split(\"|\")[-1].split(\"at\")[0].strip() if date_tag else None\n",
        "                date = None\n",
        "                if date_raw:\n",
        "                    try: date = datetime.strptime(date_raw, \"%b %d, %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception: date = date_raw\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner = BeautifulSoup(robust_get(scraper, link, headers).text, \"html.parser\")\n",
        "                    content_div = inner.find(\"div\", {\"id\": \"articleContent\"})\n",
        "                    texts = [p.get_text(\" \", strip=True) for p in content_div.find_all([\"p\", \"h2\", \"h3\", \"li\"])] if content_div else []\n",
        "                    if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_adweek_sync():\n",
        "    site, url = \"ADWEEK\", \"https://www.adweek.com/brand-marketing/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(robust_get(scraper, url, headers).text, \"html.parser\")\n",
        "        targets = []\n",
        "        hero = soup.find(\"div\", class_=\"hero__text\"); hero_a = hero.find(\"a\") if hero else None # title,link tag class 종류가 2개라 추가 과정 필요\n",
        "        if hero_a: targets.append(hero_a)\n",
        "        targets.extend(soup.find_all([\"h2\", \"h3\"], class_=[\"section__headline\", \"font-heading\", \"fw-bold\"]))\n",
        "\n",
        "        for art in targets:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                a_tag = art if art.name == \"a\" else art.find(\"a\")\n",
        "                if not a_tag or not a_tag.get(\"href\"): continue\n",
        "                title, link = a_tag.get_text(strip=True), a_tag[\"href\"]\n",
        "\n",
        "                inner = BeautifulSoup(robust_get(scraper, link, headers).text, \"html.parser\")\n",
        "                date_tag = inner.find(\"time\", class_=\"custom-publish-time\")\n",
        "                if date_tag and \"datetime\" in date_tag.attrs:\n",
        "                    date = date_tag[\"datetime\"].split(\"UTC\")[0].strip()\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    content_div = inner.select(\"div.aw-article-content p\")\n",
        "                    texts = [p.get_text(\" \", strip=True) for p in content_div] if content_div else []\n",
        "                    if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_marketingnews_sync():\n",
        "    site, url = \"Marketingnews\", \"https://www.marketingnews.es/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(robust_get(scraper, url, headers).text, \"html.parser\")\n",
        "        for art in soup.find_all(\"a\", class_=\"title\") or []:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                title_tag = art.find(\"h2\") or art.find(\"span\") or art.find(\"font\")\n",
        "                title = title_tag.get_text(strip=True) if title_tag else art.get_text(strip=True)\n",
        "                link_tag = art.get(\"href\", \"\")\n",
        "                link = f\"{url.rstrip('/')}{link_tag}\" if link_tag.startswith(\"/\") else link_tag\n",
        "\n",
        "                inner_res = robust_get(scraper, link, headers)\n",
        "                if not inner_res: continue\n",
        "                inner = BeautifulSoup(inner_res.text, \"html.parser\")\n",
        "                date_tag = inner.select_one(\"header#headDetail div.date time\")\n",
        "                if date_tag:\n",
        "                    date_raw = date_tag.get_text(strip=True)\n",
        "                    for fmt in (\"%d/%m/%Y\", \"%Y-%m-%d\", \"%d-%m-%Y\"):\n",
        "                        try:\n",
        "                            date = datetime.strptime(date_raw, fmt).strftime(\"%Y-%m-%d\")\n",
        "                            break\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    content_div = inner.select_one(\"main#bodyDetail\")\n",
        "                    if content_div:\n",
        "                        texts = [tag.get_text(strip=True, separator=\" \") for tag in content_div.find_all([\"p\",\"h2\",\"blockquote\"]) if tag.get_text(strip=True)]\n",
        "                        if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_marketingtech_sync():\n",
        "    site, url = \"MarketingTechNews\", \"https://www.marketingtechnews.net/\"\n",
        "    session, rows, errors = requests.Session(), [], []\n",
        "    session.mount('https://', TLSAdapter())\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Referer\": \"https://www.google.com/\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        res = session.get(url, headers=headers, timeout=10)\n",
        "        if res.status_code != 200:\n",
        "            raise Exception(f\"상태 코드 오류: {res.status_code}\")\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        for art in soup.find_all(\"section\", class_=\"entry-content\"):\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                a_tag = art.find(\"h3\").find(\"a\") if art.find(\"h3\") else None\n",
        "                if not a_tag: continue\n",
        "                title, link = a_tag.get_text(strip=True), a_tag.get(\"href\")\n",
        "                date_tag = art.select_one(\".byline-content-holder .content\")\n",
        "                date_raw = date_tag.get_text(strip=True).split(\"|\")[0].strip() if date_tag else None\n",
        "                if date_raw:\n",
        "                    try: date = datetime.strptime(date_raw, \"%d %B %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception: date = date_raw\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner_res = session.get(link, headers=headers, timeout=10)\n",
        "                    inner = BeautifulSoup(inner_res.text, \"html.parser\")\n",
        "                    content_div = inner.select_one(\"section.entry-content\")\n",
        "                    if content_div:\n",
        "                        texts = [t.get_text(\" \", strip=True) for t in content_div.find_all([\"p\", \"blockquote\"]) if t.get_text(strip=True)]\n",
        "                        if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_euronews_sync():\n",
        "    site, url = \"Euronews\", \"https://www.euronews.com/business\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    def abs_url(href: str) -> str:\n",
        "        href = (href or \"\").strip()\n",
        "        return href if href.startswith(\"http\") else f\"https://www.euronews.com{href}\"\n",
        "\n",
        "    def extract_article_content(inner_soup):\n",
        "        content_selectors = [\n",
        "            'div.c-article-content.c-article-content--business.js-article-content',\n",
        "            'div.c-article-content.c-article-content--business',\n",
        "            'div.c-article-content.js-article-content',\n",
        "            'div.c-article-content',\n",
        "            'article.c-article-content',\n",
        "            'div#article-content',\n",
        "        ]\n",
        "        content_div = None\n",
        "        for sel in content_selectors:\n",
        "            content_div = inner_soup.select_one(sel)\n",
        "            if content_div:\n",
        "                break\n",
        "        if not content_div:\n",
        "            return None\n",
        "\n",
        "        parts = []\n",
        "        for tag in content_div.find_all([\"p\", \"h2\", \"li\"]):\n",
        "            t = tag.get_text(\" \", strip=True)\n",
        "            if t:\n",
        "                parts.append(t)\n",
        "\n",
        "        parts = [p for i, p in enumerate(parts) if p and (i == 0 or p != parts[i-1])]\n",
        "        return \"\\n\".join(parts) if parts else None\n",
        "\n",
        "    try:\n",
        "        html = robust_get(scraper, url, headers).text\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        section = None\n",
        "        section_selectors = [\n",
        "            'section[data-block=\"topStoriesVerticalBlock\"]',\n",
        "            'div.b-top-stories-vertical',\n",
        "            'div.b-top-stories-vertical__container',\n",
        "            'main .b-top-stories-vertical',\n",
        "        ]\n",
        "        for sel in section_selectors:\n",
        "            section = soup.select_one(sel)\n",
        "            if section:\n",
        "                break\n",
        "\n",
        "        if section:\n",
        "            targets = section.select('article.the-media-object')\n",
        "        else:\n",
        "            targets = soup.select('article.the-media-object')\n",
        "\n",
        "        seen_links = set()\n",
        "\n",
        "        for art in targets:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                link_tag = art.select_one('a.the-media-object__link')\n",
        "                if not link_tag:\n",
        "                    link_tag = art.select_one('figure.the-media-object__figure a')\n",
        "\n",
        "                if link_tag and link_tag.get('href'):\n",
        "                    link = abs_url(link_tag.get('href'))\n",
        "\n",
        "                if not link or link in seen_links:\n",
        "                    continue\n",
        "                seen_links.add(link)\n",
        "\n",
        "                title_tag = art.select_one('h2.the-media-object__title')\n",
        "                if title_tag:\n",
        "                    title = title_tag.get_text(strip=True)\n",
        "                elif link_tag and link_tag.get('aria-label'):\n",
        "                    title = link_tag.get('aria-label').strip()\n",
        "\n",
        "                inner = BeautifulSoup(scraper.get(link, headers=headers).text, \"html.parser\")\n",
        "                date_raw = None; date = None\n",
        "                date_tag = inner.find('time')\n",
        "                date_raw = date_tag.get('datetime')\n",
        "                date = date_raw.split(' ')[0]\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    content_text = extract_article_content(inner)\n",
        "                    if content_text: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": content_text, \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_thenextweb_sync():\n",
        "    site, url = \"The Next Web\", \"https://thenextweb.com/latest\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        res = scraper.get(url, headers=headers)\n",
        "        res.raise_for_status()\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        targets = []\n",
        "        for a in soup.select(\"article.c-topArticles__article header h2 a\") + soup.select(\"article.c-listArticle h2 a\"):\n",
        "            if a and a.get(\"href\"):\n",
        "                title, link = a.get_text(strip=True), f\"https://thenextweb.com{a.get('href')}\"\n",
        "                if link not in [art[\"link\"] for art in targets]:\n",
        "                    targets.append({\"title\": title, \"link\": link})\n",
        "\n",
        "        for art in targets:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                inner_res = scraper.get(art[\"link\"], headers=headers)\n",
        "                inner_res.raise_for_status()\n",
        "                inner = BeautifulSoup(inner_res.text, \"html.parser\")\n",
        "\n",
        "                time_tag = inner.select_one(\"footer.c-article__pubDate time\")\n",
        "                if time_tag:\n",
        "                    date_raw = time_tag.get_text(strip=True)\n",
        "                    date_str = date_raw.split(\"-\")[0].strip()\n",
        "                    try:\n",
        "                        date = datetime.strptime(date_str, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception:\n",
        "                        date = date_raw\n",
        "\n",
        "                if art[\"title\"]: count_title += 1\n",
        "                if art[\"link\"]: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    content_div = inner.select_one(\"div#article-main-content\")\n",
        "                    if content_div:\n",
        "                        for ad in content_div.select(\"div.channel-cta-wrapper, div[id^='hs-embed'], aside, div[class*='ad'], div[class*='sponsor'], div[class*='newsletter']\"):\n",
        "                            ad.decompose()\n",
        "                        texts = [p.get_text(\" \", strip=True) for p in content_div.find_all(\"p\") if p.get_text(strip=True)]\n",
        "                        if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": art[\"title\"], \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": art[\"link\"]})\n",
        "            except Exception as e:\n",
        "                errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url,\n",
        "                                \"count_title\": 0, \"count_link\": 0, \"count_date\": 0,\n",
        "                                \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_ainews_sync():\n",
        "    site, url = \"AI NEWS\", \"https://www.artificialintelligence-news.com/all-categories/\"\n",
        "    warnings.simplefilter(\"ignore\", InsecureRequestWarning)\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.set_ciphers('DEFAULT:@SECLEVEL=1')\n",
        "    scraper, rows, errors = cloudscraper.create_scraper(browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"mobile\": False}, ssl_context=ssl_context),[], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        response = scraper.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        articles = soup.select(\"div.e-loop-item\")\n",
        "\n",
        "        for art in articles:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                title_tag = art.select_one(\".elementor-widget-theme-post-title h1 a\")\n",
        "                title = title_tag.get_text(strip=True) if title_tag else None\n",
        "                link = title_tag[\"href\"] if title_tag and title_tag.has_attr(\"href\") else None\n",
        "\n",
        "                date_tag = art.select_one(\".elementor-element-18b13528 .elementor-heading-title\")\n",
        "                date_raw = date_tag.get_text(strip=True) if date_tag else None\n",
        "                if date_raw:\n",
        "                    try: date = datetime.strptime(date_raw, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception: date = date_raw\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner_context = ssl.create_default_context(); inner_context.set_ciphers('DEFAULT:@SECLEVEL=1'); inner_context.check_hostname = False\n",
        "                    inner_scraper = cloudscraper.create_scraper(browser={\"browser\": \"chrome\", \"platform\": \"windows\", \"mobile\": False}, ssl_context=inner_context)\n",
        "                    inner_res = inner_scraper.get(link, verify=False, headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/128.0\"})\n",
        "                    inner = BeautifulSoup(inner_res.text, \"html.parser\")\n",
        "                    content_div = inner.select_one(\"div.elementor-widget-theme-post-content\")\n",
        "                    if content_div:\n",
        "                        texts = [p.get_text(\" \", strip=True) for p in content_div.find_all([\"p\", \"h3\"]) if p.get_text(strip=True)]\n",
        "                        if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url,\n",
        "                                \"count_title\": 0, \"count_link\": 0, \"count_date\": 0,\n",
        "                                \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_socialmedia_sync():\n",
        "    site, url = \"Social Media Today\", \"https://www.socialmediatoday.com/\"\n",
        "    scraper, rows, errors = create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(scraper.get(url, headers=headers).text, \"html.parser\")\n",
        "        feed_targets = soup.find(\"section\", class_=\"dash-feed\")\n",
        "        targets = feed_targets.find_all(\"li\", class_=\"row feed__item\", recursive=True) if feed_targets else []\n",
        "\n",
        "        for art in targets:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                if \"feed-item-ad\" in art.get(\"class\", []): continue\n",
        "                title_tag = art.find(\"h3\", class_=\"feed__title\");\n",
        "                if not title_tag: continue\n",
        "                link_tag = title_tag.find(\"a\")\n",
        "                title, link = link_tag.get_text(strip=True), link_tag[\"href\"] if link_tag else None\n",
        "                if link and link.startswith(\"/\"): link = f\"https://www.socialmediatoday.com{link}\"\n",
        "\n",
        "                inner = BeautifulSoup(scraper.get(link, headers=headers).text, \"html.parser\")\n",
        "                date_raw = None; date = None\n",
        "                date_tag = inner.find(\"span\", class_=\"published-info\")\n",
        "                if date_tag:\n",
        "                    text = date_tag.get_text(strip=True)\n",
        "                    m = re.search(r\"([A-Za-z]+\\.\\s?\\d{1,2},\\s?\\d{4})\", text)\n",
        "                    if m:\n",
        "                        date_raw = m.group(1).replace(\".\", \"\")\n",
        "                        date = datetime.strptime(date_raw, \"%b %d, %Y\").strftime(\"%Y-%m-%d\")\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    content_div = inner.find(\"div\", class_=\"article-body\")\n",
        "                    if content_div:\n",
        "                        for ad in content_div.find_all([\"div\", \"script\", \"form\"], recursive=True): ad.decompose()\n",
        "                        texts = [p.get_text(\" \", strip=True) for p in content_div.find_all(\"p\") if p.get_text(strip=True)]\n",
        "                        if texts: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": \"\\n\".join(texts), \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "def crawl_einpresswire_sync():\n",
        "    _mojibake_signatures = (\n",
        "        \"Ã\", \"Â\", \"â€”\", \"â€“\", \"â€\", \"â€œ\", \"â€˜\", \"â€™\", \"â€¦\", \"�\"\n",
        "    )\n",
        "    def fix_encoding(text: str) -> str:\n",
        "        if not text:\n",
        "            return text\n",
        "\n",
        "        if any(sig in text for sig in _mojibake_signatures):\n",
        "            try:\n",
        "                fixed = text.encode(\"latin1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
        "                if fixed.count(\"�\") < text.count(\"�\"):\n",
        "                    return fixed\n",
        "\n",
        "                def _korean_ratio(s):\n",
        "                    total = len(s)\n",
        "                    if total == 0: return 0.0\n",
        "                    ko = sum(0xAC00 <= ord(ch) <= 0xD7A3 for ch in s)\n",
        "                    return ko / total\n",
        "                if _korean_ratio(fixed) >= _korean_ratio(text):\n",
        "                    return fixed\n",
        "            except Exception:\n",
        "                pass\n",
        "        return text\n",
        "\n",
        "    site, url = \"EIN Presswire\", \"https://www.einpresswire.com/channel/media-advertising-pr#\"\n",
        "    scraper, rows, errors = cloudscraper.create_scraper(), [], []\n",
        "    count_title = count_link = count_date = count_content = matched = 0\n",
        "\n",
        "    try:\n",
        "        response = scraper.get(url, headers=headers); response.encoding = \"utf-8\"\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        articles = soup.select(\"ul.pr-feed li.funlist0\")\n",
        "\n",
        "        for art in articles:\n",
        "            try:\n",
        "                title = link = date = None\n",
        "                title_tag, date_tag = art.select_one(\"h3 a\"), art.select_one(\".pretitle .date a\")\n",
        "                if not title_tag: continue\n",
        "                title, link = title_tag.get_text(strip=True), title_tag[\"href\"]\n",
        "\n",
        "                date_raw = date_tag.get_text(strip=True) if date_tag else None\n",
        "                date = None\n",
        "                if date_raw:\n",
        "                    try: date = datetime.strptime(date_raw, \"%B %d, %Y\").strftime(\"%Y-%m-%d\")\n",
        "                    except Exception: date = date_raw\n",
        "\n",
        "                if title: count_title += 1\n",
        "                if link: count_link += 1\n",
        "                if date: count_date += 1\n",
        "\n",
        "                if date == yesterday_str:\n",
        "                    matched += 1\n",
        "                    inner_res = robust_get(scraper, link, headers)\n",
        "                    inner_res.encoding = \"utf-8\"\n",
        "                    inner = BeautifulSoup(inner_res.text, \"html.parser\")\n",
        "                    content_div = inner.select_one(\"div.article_column.imported\")\n",
        "                    if content_div:\n",
        "                        for t in content_div.select(\"script, iframe, img, style\"): t.decompose()\n",
        "                        for p in content_div.select(\"p\"):\n",
        "                            text = p.get_text(strip=True)\n",
        "                            if text.startswith(\"Legal Disclaimer\") or p.get(\"class\") in [[\"contact\"], [\"pr_video_title\"]]:\n",
        "                                p.decompose()\n",
        "                        f = content_div.select_one(\"div.article-footer\")\n",
        "                        if f: f.decompose()\n",
        "\n",
        "                        content = content_div.get_text(separator=\"\\n\", strip=True)\n",
        "                        content = fix_encoding(content)\n",
        "                        if content: count_content += 1; rows.append({\"매체명\": site, \"제목\": title, \"날짜\": date, \"내용\": content, \"링크\": link})\n",
        "            except Exception as e: errors.append(f\"개별 기사 오류: {e}\")\n",
        "\n",
        "        meta = {\"site\": site, \"url\": url, \"count_title\": count_title, \"count_link\": count_link,\n",
        "                \"count_date\": count_date, \"count_content\": count_content, \"matched\": matched, \"errors\": errors}\n",
        "        return pd.DataFrame(rows), meta\n",
        "\n",
        "    except Exception as e:\n",
        "        return pd.DataFrame(), {\"site\": site, \"url\": url, \"count_title\": 0, \"count_link\": 0,\n",
        "                                \"count_date\": 0, \"count_content\": 0, \"matched\": 0, \"errors\": [f\"메인 로드 실패: {e}\"]}\n",
        "\n",
        "# =========================\n",
        "# 실행\n",
        "# =========================\n",
        "\n",
        "async def run_all_crawlers():\n",
        "    parallel_crawlers = [crawl_marketingmag_sync,\n",
        "                         crawl_marketingbeat_sync,\n",
        "                         crawl_searchengine_sync,\n",
        "                         crawl_adweek_sync,\n",
        "                         crawl_marketingnews_sync,\n",
        "                         crawl_euronews_sync,\n",
        "                         crawl_thenextweb_sync,\n",
        "                         crawl_socialmedia_sync,\n",
        "                         crawl_einpresswire_sync,\n",
        "                         crawl_ainews_sync]\n",
        "    sequential_crawlers = [crawl_marketingtech_sync]\n",
        "\n",
        "    tasks = [asyncio.to_thread(lambda c=c: c()) for c in parallel_crawlers]\n",
        "    parallel_results = await asyncio.gather(*tasks)\n",
        "\n",
        "    sequential_results = []\n",
        "    for c in sequential_crawlers:\n",
        "        res = await asyncio.to_thread(lambda c=c: c())\n",
        "        sequential_results.append(res)\n",
        "        await asyncio.sleep(random.uniform(0.3,0.6))\n",
        "\n",
        "    results = parallel_results+sequential_results\n",
        "    all_rows, warnings, metas = [], [], []\n",
        "    for df, meta in results:\n",
        "        metas.append(meta)\n",
        "        if should_warn(meta): warnings.append(make_warning_row(meta, yesterday_str))\n",
        "        if not df.empty: all_rows.append(df)\n",
        "\n",
        "    df_data = pd.concat(all_rows, ignore_index=True) if all_rows else pd.DataFrame(columns=[\"매체명\",\"제목\",\"날짜\",\"내용\",\"링크\"])\n",
        "    df_warn = pd.concat(warnings, ignore_index=True) if warnings else pd.DataFrame(columns=[\"매체명\",\"제목\",\"날짜\",\"내용\",\"링크\"])\n",
        "    return df_data, df_warn, metas\n",
        "\n",
        "df_data, df_warn, metas = asyncio.get_event_loop().run_until_complete(run_all_crawlers())\n",
        "\n",
        "# =========================\n",
        "# 시트 업데이트\n",
        "# =========================\n",
        "gc = gspread.service_account(\"{json 파일명}\") # 구글 api 사용을 위해 필요한 json 키 값\n",
        "spreadsheet = gc.open_by_key(\"{스프레드시트 주소}\")\n",
        "worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "existing = worksheet.get_all_values()\n",
        "next_row = len(existing) + 1\n",
        "\n",
        "blocks = []\n",
        "if not df_data.empty: blocks.append(df_data)\n",
        "if not df_warn.empty: blocks.append(df_warn)\n",
        "if blocks:\n",
        "    df_out = pd.concat(blocks, ignore_index=True)\n",
        "    df_upload = df_out.drop(columns=[\"내용\"])\n",
        "    set_with_dataframe(worksheet, df_upload, row=next_row, col=1, include_column_header=False)\n",
        "\n",
        "print(\"=== METAS ===\")\n",
        "for m in metas:\n",
        "    print(m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7891613f",
      "metadata": {
        "id": "7891613f"
      },
      "outputs": [],
      "source": [
        "# 챗 gpt 번역 프로그램\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"{gpt api 키 값}\" # 챗 gpt api 사용을 위해 필요한 키 값\n",
        "\n",
        "def translate_long_text(text, target_language=\"Korean\"):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    translated_paragraphs = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        if paragraph.strip():\n",
        "            try:\n",
        "                response = openai.chat.completions.create(\n",
        "                    model=\"gpt-4o-mini\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": f\"You are a helpful assistant that translates text to {target_language}.\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"Please translate the following text into {target_language}:\\n\\n{paragraph}\"}\n",
        "                    ]\n",
        "                )\n",
        "                translated_paragraphs.append(response.choices[0].message.content.strip())\n",
        "            except Exception as e:\n",
        "                print(f\"Error during translation: {e}\")\n",
        "                translated_paragraphs.append(paragraph)\n",
        "\n",
        "    return \"\\n\\n\".join(translated_paragraphs)\n",
        "\n",
        "translated_data = []\n",
        "\n",
        "for idx, row in df_out.iterrows():\n",
        "    date = row['날짜']\n",
        "    title = row['제목']\n",
        "    content = row['내용']\n",
        "\n",
        "    translated_title = translate_long_text(str(title), target_language=\"Korean\")\n",
        "    translated_content = translate_long_text(str(content), target_language=\"Korean\")\n",
        "\n",
        "    translated_data.append({\n",
        "        \"날짜\": date,\n",
        "        \"제목\": translated_title,\n",
        "        \"내용\": translated_content\n",
        "    })\n",
        "\n",
        "translated_df = pd.DataFrame(translated_data)\n",
        "\n",
        "gc = gspread.service_account(\"{json 파일명}\") # 구글 api 사용을 위해 필요한 json 키 값\n",
        "spreadsheet = gc.open_by_key(\"{스프레드시트 주소}\")\n",
        "worksheet = spreadsheet.worksheet(\"{스프레드시트 셀 이름}\")\n",
        "existing = worksheet.get_all_values()\n",
        "next_row = len(existing) + 1\n",
        "set_with_dataframe(worksheet, translated_df, row=next_row, col=1, include_column_header=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
